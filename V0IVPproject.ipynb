{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx3XDiuQB4lQ",
        "outputId": "b8b21f03-d1f9-40de-a68f-2f520c431629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ZIP_PATH = \"/content/drive/MyDrive/B. Disease Grading.zip\"\n"
      ],
      "metadata": {
        "id": "7kmlZo6oCi3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"$ZIP_PATH\" -d \"/content\"\n",
        "print(\"Extraction completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpuVCFHDCoUF",
        "outputId": "565c9d86-5329-4c33-f0d3-f7324a69ca89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = \"/content/B. Disease Grading\"\n",
        "\n",
        "TRAIN_IMG = BASE + \"/1. Original Images/a. Training Set\"\n",
        "TEST_IMG  = BASE + \"/1. Original Images/b. Testing Set\"\n",
        "\n",
        "TRAIN_CSV = BASE + \"/2. Groundtruths/a. IDRiD_Disease Grading_Training Labels.csv\"\n",
        "TEST_CSV  = BASE + \"/2. Groundtruths/b. IDRiD_Disease Grading_Testing Labels.csv\"\n",
        "\n",
        "print(\"Train images folder:\", TRAIN_IMG)\n",
        "print(\"Test images folder:\", TEST_IMG)\n",
        "print(\"Train CSV:\", TRAIN_CSV)\n",
        "print(\"Test CSV:\", TEST_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHRZ536eCwKN",
        "outputId": "297f2cd7-6fb2-4412-b96d-38797b0473cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images folder: /content/B. Disease Grading/1. Original Images/a. Training Set\n",
            "Test images folder: /content/B. Disease Grading/1. Original Images/b. Testing Set\n",
            "Train CSV: /content/B. Disease Grading/2. Groundtruths/a. IDRiD_Disease Grading_Training Labels.csv\n",
            "Test CSV: /content/B. Disease Grading/2. Groundtruths/b. IDRiD_Disease Grading_Testing Labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python scikit-image scikit-learn pywt tqdm matplotlib seaborn --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7e9Yh4yDngJ",
        "outputId": "0a9e68aa-6119-418e-caaa-a9a7ae5e82f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pywt (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pywt\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5lBJ-VNJDodV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpgradedDRClassifier:\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.clf = None\n",
        "        self.pca = None\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # PREPROCESSING\n",
        "    # ------------------------------------------------------------\n",
        "    def preprocess(self, img):\n",
        "        # Resize\n",
        "        img = cv2.resize(img, (512, 512))\n",
        "\n",
        "        # -------------------------------------\n",
        "        # 1. Illumination Normalization\n",
        "        # -------------------------------------\n",
        "        img_float = img.astype(np.float32)\n",
        "\n",
        "        # Remove uneven illumination\n",
        "        background = cv2.GaussianBlur(img_float, (75, 75), 0)\n",
        "        img_norm = cv2.divide(img_float, background + 1e-6, scale=255)\n",
        "\n",
        "        img_norm = np.clip(img_norm, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # -------------------------------------\n",
        "        # 2. Histogram Equalization (per channel)\n",
        "        # -------------------------------------\n",
        "        ycrcb = cv2.cvtColor(img_norm, cv2.COLOR_BGR2YCrCb)\n",
        "        y, cr, cb = cv2.split(ycrcb)\n",
        "\n",
        "        y_eq = cv2.equalizeHist(y)\n",
        "        img_eq = cv2.cvtColor(cv2.merge([y_eq, cr, cb]), cv2.COLOR_YCrCb2BGR)\n",
        "\n",
        "        # -------------------------------------\n",
        "        # 3. CLAHE (Local Contrast)\n",
        "        # -------------------------------------\n",
        "        lab = cv2.cvtColor(img_eq, cv2.COLOR_BGR2LAB)\n",
        "        l, a, b = cv2.split(lab)\n",
        "\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        l = clahe.apply(l)\n",
        "\n",
        "        enhanced = cv2.cvtColor(cv2.merge([l, a, b]), cv2.COLOR_LAB2BGR)\n",
        "\n",
        "        return enhanced\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # MACULA / FOVEA DETECTION (BRIGHTNESS VALLEY SEARCH)\n",
        "    # ------------------------------------------------------------\n",
        "    def detect_macula(self, img):\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        blur = cv2.GaussianBlur(gray, (45, 45), 0)\n",
        "\n",
        "        # macula = darkest point near center; use weighted center search\n",
        "        h, w = gray.shape\n",
        "        cx, cy = w // 2, h // 2\n",
        "\n",
        "        crop = blur[cy-120:cy+120, cx-120:cx+120]\n",
        "        min_loc = np.unravel_index(np.argmin(crop), crop.shape)\n",
        "\n",
        "        mac_y = cy - 120 + min_loc[0]\n",
        "        mac_x = cx - 120 + min_loc[1]\n",
        "\n",
        "        return (mac_x, mac_y)\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # MACULA REGION EXUDATE DENSITY\n",
        "    # ------------------------------------------------------------\n",
        "    def macula_exudate_density(self, ex_mask, macula_coord):\n",
        "        x, y = macula_coord\n",
        "        r = 60  # radius around macula\n",
        "\n",
        "        h, w = ex_mask.shape\n",
        "        mask = np.zeros((h, w), np.uint8)\n",
        "\n",
        "        cv2.circle(mask, (x, y), r, 255, -1)\n",
        "\n",
        "        macula_region = cv2.bitwise_and(ex_mask, mask)\n",
        "        density = np.sum(macula_region > 0) / (np.pi * r * r)\n",
        "\n",
        "        return density\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # VESSEL DETECTION (simplified)\n",
        "    # ------------------------------------------------------------\n",
        "    def vessel_mask(self, img):\n",
        "        green = img[:, :, 1]\n",
        "        green = cv2.createCLAHE(2.0, (8, 8)).apply(green)\n",
        "        bg = cv2.morphologyEx(green, cv2.MORPH_OPEN, np.ones((5, 5), np.uint8))\n",
        "        vessel = cv2.subtract(green, bg)\n",
        "        _, mask = cv2.threshold(vessel, 15, 255, cv2.THRESH_BINARY)\n",
        "        return mask\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # LESION DETECTION (MA, HE, EX)\n",
        "    # ------------------------------------------------------------\n",
        "    def detect_ma(self, img):\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, np.ones((7, 7), np.uint8))\n",
        "        _, mask = cv2.threshold(tophat, 10, 255, cv2.THRESH_BINARY)\n",
        "        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        count, area = 0, 0\n",
        "        for c in cnts:\n",
        "            a = cv2.contourArea(c)\n",
        "            if 5 < a < 120:\n",
        "                count += 1\n",
        "                area += a\n",
        "        return count, area, mask\n",
        "\n",
        "    def detect_he(self, img):\n",
        "        red = img[:, :, 2]\n",
        "        bg = cv2.morphologyEx(red, cv2.MORPH_OPEN, np.ones((7, 7), np.uint8))\n",
        "        diff = cv2.subtract(red, bg)\n",
        "        _, mask = cv2.threshold(diff, 20, 255, cv2.THRESH_BINARY)\n",
        "        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        count, area = 0, 0\n",
        "        for c in cnts:\n",
        "            a = cv2.contourArea(c)\n",
        "            if a > 40:\n",
        "                count += 1\n",
        "                area += a\n",
        "        return count, area, mask\n",
        "\n",
        "    def detect_ex(self, img):\n",
        "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "        lower = np.array([20, 40, 150])\n",
        "        upper = np.array([35, 255, 255])\n",
        "        mask = cv2.inRange(hsv, lower, upper)\n",
        "        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        count, area = 0, 0\n",
        "        for c in cnts:\n",
        "            a = cv2.contourArea(c)\n",
        "            if a > 40:\n",
        "                count += 1\n",
        "                area += a\n",
        "        return count, area, mask\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # GLCM TEXTURE FEATURES\n",
        "    # ------------------------------------------------------------\n",
        "    def glcm_features(self, gray):\n",
        "        glcm = graycomatrix(gray, distances=[2], angles=[0],\n",
        "                            symmetric=True, normed=True)\n",
        "\n",
        "        feats = [\n",
        "            graycoprops(glcm, 'contrast')[0][0],\n",
        "            graycoprops(glcm, 'homogeneity')[0][0],\n",
        "            graycoprops(glcm, 'energy')[0][0],\n",
        "            graycoprops(glcm, 'correlation')[0][0],\n",
        "            graycoprops(glcm, 'dissimilarity')[0][0]\n",
        "        ]\n",
        "\n",
        "        return feats\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # LBP FEATURES\n",
        "    # ------------------------------------------------------------\n",
        "    def lbp_features(self, gray):\n",
        "        radius = 2\n",
        "        points = radius * 8\n",
        "        lbp = local_binary_pattern(gray, points, radius, method='uniform')\n",
        "        hist, _ = np.histogram(lbp.ravel(), bins=16, range=(0, 16), density=True)\n",
        "        return hist.tolist()\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # MAIN FEATURE VECTOR\n",
        "    # ------------------------------------------------------------\n",
        "    def extract_features(self, img_path):\n",
        "        img = cv2.imread(img_path)\n",
        "        enh = self.preprocess(img)\n",
        "        gray = cv2.cvtColor(enh, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # --- Lesions ---\n",
        "        ma_c, ma_a, ma_mask = self.detect_ma(enh)\n",
        "        he_c, he_a, he_mask = self.detect_he(enh)\n",
        "        ex_c, ex_a, ex_mask = self.detect_ex(enh)\n",
        "\n",
        "        # --- Macula features ---\n",
        "        mac = self.detect_macula(enh)\n",
        "        mac_ex_density = self.macula_exudate_density(ex_mask, mac)\n",
        "\n",
        "        # --- Texture ---\n",
        "        glcm = self.glcm_features(gray)\n",
        "        lbp = self.lbp_features(gray)\n",
        "\n",
        "        # --- Vessel density ---\n",
        "        vessel = self.vessel_mask(enh)\n",
        "        vessel_density = np.sum(vessel > 0) / (512 * 512)\n",
        "\n",
        "        # --- Final Feature Vector ---\n",
        "        features = np.array([\n",
        "            ma_c, ma_a,\n",
        "            he_c, he_a,\n",
        "            ex_c, ex_a,\n",
        "            mac_ex_density,\n",
        "            vessel_density,\n",
        "            *glcm,\n",
        "            *lbp\n",
        "        ])\n",
        "\n",
        "        return features\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # TRAIN RANDOM FOREST\n",
        "    # ------------------------------------------------------------\n",
        "    def train_rf(self, X_train, y_train):\n",
        "        self.clf = RandomForestClassifier(\n",
        "            n_estimators=300,\n",
        "            max_depth=15,\n",
        "            min_samples_leaf=3,\n",
        "            min_samples_split=4,\n",
        "            class_weight=None,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.clf.fit(X_train, y_train)\n",
        "        print(\"Random Forest training completed!\")\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # PREDICT\n",
        "    # ------------------------------------------------------------\n",
        "    def predict(self, X):\n",
        "        return self.clf.predict(X)"
      ],
      "metadata": {
        "id": "HoAancSmDyC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSVs\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "# Normalize column names if needed\n",
        "train_df.rename(columns={\"Image name\": \"image\", \"Retinopathy grade\": \"level\"}, inplace=True)\n",
        "test_df.rename(columns={\"Image name\": \"image\", \"Retinopathy grade\": \"level\"}, inplace=True)\n",
        "\n",
        "# Keep only needed columns\n",
        "train_df = train_df[[\"image\", \"level\"]]\n",
        "test_df  = test_df[[\"image\", \"level\"]]\n",
        "\n",
        "# Ensure .jpg extension exists\n",
        "train_df[\"image\"] = train_df[\"image\"].apply(lambda x: x if x.lower().endswith(\".jpg\") else x + \".jpg\")\n",
        "test_df[\"image\"]  = test_df[\"image\"].apply(lambda x: x if x.lower().endswith(\".jpg\")  else x + \".jpg\")\n",
        "\n",
        "print(\"Train samples:\", len(train_df))\n",
        "print(\"Test samples:\", len(test_df))\n",
        "\n",
        "train_df.head(), test_df.head()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLWXDvH0EFuZ",
        "outputId": "5ba15f13-7e43-4ec0-bb30-78c761cb8cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 413\n",
            "Test samples: 103\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(           image  level\n",
              " 0  IDRiD_001.jpg      3\n",
              " 1  IDRiD_002.jpg      3\n",
              " 2  IDRiD_003.jpg      2\n",
              " 3  IDRiD_004.jpg      3\n",
              " 4  IDRiD_005.jpg      4,\n",
              "            image  level\n",
              " 0  IDRiD_001.jpg      4\n",
              " 1  IDRiD_002.jpg      4\n",
              " 2  IDRiD_003.jpg      4\n",
              " 3  IDRiD_004.jpg      4\n",
              " 4  IDRiD_005.jpg      4)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = UpgradedDRClassifier()\n"
      ],
      "metadata": {
        "id": "gj9jUztBKjvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "print(\"Extracting TRAIN features...\")\n",
        "\n",
        "for i, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    img_path = os.path.join(TRAIN_IMG, row['image'])\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        print(\"Missing:\", img_path)\n",
        "        continue\n",
        "\n",
        "    feats = clf.extract_features(img_path)\n",
        "    X_train.append(feats)\n",
        "    y_train.append(row['level'])\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "print(\"Training feature shape:\", X_train.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp7FxLxiFNgc",
        "outputId": "18634cb0-cc70-45dc-c667-97dc74794885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting TRAIN features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 413/413 [02:08<00:00,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training feature shape: (413, 29)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "print(\"Extracting TEST features...\")\n",
        "\n",
        "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    img_path = os.path.join(TEST_IMG, row['image'])\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        print(\"Missing:\", img_path)\n",
        "        continue\n",
        "\n",
        "    feats = clf.extract_features(img_path)\n",
        "    X_test.append(feats)\n",
        "    y_test.append(row['level'])\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(\"Testing feature shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh7buqErGs7J",
        "outputId": "7160c019-d131-44a2-c076-e3381ed134e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting TEST features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [01:01<00:00,  1.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing feature shape: (103, 29)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.scaler.fit(X_train)\n",
        "\n",
        "X_train_scaled = clf.scaler.transform(X_train)\n",
        "X_test_scaled  = clf.scaler.transform(X_test)\n",
        "\n",
        "# Scale using ONLY the BALANCED dataset\n"
      ],
      "metadata": {
        "id": "D2yad48sGy44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# SMOTE Oversampling (to fix class imbalance)\n",
        "# ----------------------------------------\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "sm = SMOTE(k_neighbors=3, random_state=42)\n",
        "X_train_bal, y_train_bal = sm.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Before SMOTE:\", X_train_scaled.shape, np.bincount(y_train))\n",
        "print(\"After SMOTE:\",  X_train_bal.shape,  np.bincount(y_train_bal))\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3. SCALE AGAIN (USING BALANCED DATA)\n",
        "# ----------------------------------------\n",
        "\n",
        "clf.scaler.fit(X_train_bal)\n",
        "\n",
        "X_train_bal_scaled = clf.scaler.transform(X_train_bal)\n",
        "X_test_scaled      = clf.scaler.transform(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-0-CQOmNK5g",
        "outputId": "1d748705-4148-47f2-833d-a3a8892b4702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before SMOTE: (413, 29) [134  20 136  74  49]\n",
            "After SMOTE: (680, 29) [136 136 136 136 136]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=15, random_state=42)\n",
        "pca.fit(X_train_bal_scaled)\n",
        "\n",
        "X_train_pca = pca.transform(X_train_bal_scaled)\n",
        "X_test_pca  = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"PCA shapes:\", X_train_pca.shape, X_test_pca.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu_5spv1XWyR",
        "outputId": "ebae3a15-996e-4d5d-e344-0268a02a1d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA shapes: (680, 15) (103, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "feat_scaler = StandardScaler()\n",
        "feat_scaler.fit(X_train_pca)\n",
        "\n",
        "X_train_final = feat_scaler.transform(X_train_pca)\n",
        "X_test_final  = feat_scaler.transform(X_test_pca)\n"
      ],
      "metadata": {
        "id": "cDXgb9xpaIHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_temp = SVC(\n",
        "    kernel='rbf',\n",
        "    C=8,\n",
        "    gamma='scale',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores = cross_val_score(\n",
        "    svm_temp,\n",
        "    X_train_pca,          # PCA-transformed balanced data\n",
        "    y_train_bal,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "print(\"SVM CV scores:\", scores)\n",
        "print(\"Mean SVM CV accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w12VhzEcZGYK",
        "outputId": "a999888a-bb6a-413c-e214-c1a690b11603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM CV scores: [0.49264706 0.48529412 0.61029412 0.61764706 0.61029412]\n",
            "Mean SVM CV accuracy: 0.5632352941176471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf = SVC(\n",
        "    kernel='rbf',\n",
        "    C=8,\n",
        "    gamma='scale',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "svm_clf.fit(X_train_pca, y_train_bal)\n",
        "print(\"Final SVM trained!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gmGHRzaZIMb",
        "outputId": "3ec7d8f4-b850-423b-f8cf-f23f720d2b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final SVM trained!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "train_pred = svm_clf.predict(X_train_pca)\n",
        "print(\"Training Accuracy:\", accuracy_score(y_train_bal, train_pred))\n",
        "print(\"\\nTraining Report:\\n\")\n",
        "print(classification_report(y_train_bal, train_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22G-pfY_ZMwN",
        "outputId": "35c7e466-e1f0-4bf0-da52-97896d6fe17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.3352941176470588\n",
            "\n",
            "Training Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.57      0.41       136\n",
            "           1       0.00      0.00      0.00       136\n",
            "           2       0.34      0.61      0.44       136\n",
            "           3       0.28      0.35      0.31       136\n",
            "           4       0.68      0.15      0.25       136\n",
            "\n",
            "    accuracy                           0.34       680\n",
            "   macro avg       0.32      0.34      0.28       680\n",
            "weighted avg       0.32      0.34      0.28       680\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = svm_clf.predict(X_test_pca)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, test_pred))\n",
        "print(\"\\nTesting Report:\\n\")\n",
        "print(classification_report(y_test, test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqoaOFgnZTBx",
        "outputId": "e6c6570c-c709-4fe0-b6de-46c2ec618890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.3106796116504854\n",
            "\n",
            "Testing Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        34\n",
            "           1       0.00      0.00      0.00         5\n",
            "           2       0.31      1.00      0.47        32\n",
            "           3       0.00      0.00      0.00        19\n",
            "           4       0.00      0.00      0.00        13\n",
            "\n",
            "    accuracy                           0.31       103\n",
            "   macro avg       0.06      0.20      0.09       103\n",
            "weighted avg       0.10      0.31      0.15       103\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_clf = SVC(\n",
        "    kernel='rbf',\n",
        "    C=8,                   # good for noisy DR data\n",
        "    gamma='scale',\n",
        "    class_weight='balanced',   # important\n",
        "    probability=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "svm_clf.fit(X_train_pca, y_train_bal)\n",
        "print(\"SVM (RBF) training completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1Z8xQRGXaFU",
        "outputId": "62858b26-f79a-4da1-ac66-79bdf4ee3dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM (RBF) training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "train_pred = svm_clf.predict(X_train_pca)\n",
        "\n",
        "print(\"Training Accuracy:\", accuracy_score(y_train_bal, train_pred))\n",
        "print(\"\\nTraining Classification Report:\\n\")\n",
        "print(classification_report(y_train_bal, train_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtEKkDHUXAF6",
        "outputId": "c509cb1f-2696-48cf-dbae-fcbbcb2f5f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.7823529411764706\n",
            "\n",
            "Training Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.74      0.78       136\n",
            "           1       0.81      0.98      0.89       136\n",
            "           2       0.74      0.58      0.65       136\n",
            "           3       0.73      0.75      0.74       136\n",
            "           4       0.79      0.87      0.83       136\n",
            "\n",
            "    accuracy                           0.78       680\n",
            "   macro avg       0.78      0.78      0.78       680\n",
            "weighted avg       0.78      0.78      0.78       680\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = svm_clf.predict(X_test_pca)\n",
        "\n",
        "print(\"Testing Accuracy:\", accuracy_score(y_test, test_pred))\n",
        "print(\"\\nTesting Classification Report:\\n\")\n",
        "print(classification_report(y_test, test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxJ5M4DFXhmF",
        "outputId": "dc5daf9a-0d08-4b89-cc0c-420a273b512d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: 0.3300970873786408\n",
            "\n",
            "Testing Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      1.00      0.50        34\n",
            "           1       0.00      0.00      0.00         5\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.00      0.00      0.00        19\n",
            "           4       0.00      0.00      0.00        13\n",
            "\n",
            "    accuracy                           0.33       103\n",
            "   macro avg       0.07      0.20      0.10       103\n",
            "weighted avg       0.11      0.33      0.16       103\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}